{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/eda_dataset_imputed.csv')\n",
    "\n",
    "app_train, test_1 = train_test_split(data, test_size=0.30, random_state=64)\n",
    "app_test, app_validation = train_test_split(test_1, test_size=0.5, random_state=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = app_train['TARGET']\n",
    "test_labels = app_test['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (215257, 449)\n",
      "Testing data shape:  (46127, 449)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "\n",
    "# Drop the target from the training data\n",
    "if 'TARGET' in app_train:\n",
    "    train = app_train.drop(['TARGET'], axis=1)\n",
    "else:\n",
    "    train = app_train.copy()\n",
    "\n",
    "    \n",
    "if 'TARGET' in app_test:\n",
    "    test = app_test.drop(['TARGET'], axis=1)\n",
    "else:\n",
    "    test = app_test.copy()\n",
    "\n",
    "    \n",
    "# Feature names\n",
    "features = list(train.columns)\n",
    "\n",
    "# Median imputation of missing values\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "# Scale each feature to 0-1\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "# Fit on the training data\n",
    "imputer.fit(train)\n",
    "\n",
    "# Transform both training and testing data\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(test)\n",
    "\n",
    "# Repeat with the scaler\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "print('Training data shape: ', train.shape)\n",
    "print('Testing data shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "\n",
    "import gc\n",
    "\n",
    "def model(features, test_features, params, n_folds = 5): \n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = np.array(features['TARGET'].astype(int))\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    features = features.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
    "    test_features = test_features.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
    "    \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        # Create the model\n",
    "        model = xgb.XGBClassifier(learning_rate =params[\"learning_rate\"], \n",
    "                                  n_estimators=params[\"n_estimators\"], \n",
    "                                  max_depth=params[\"max_depth\"], \n",
    "                                  min_child_weight=params[\"min_child_weight\"], \n",
    "                                  subsample=params[\"subsample\"], \n",
    "                                  colsample_bytree=params[\"colsample_bytree\"], \n",
    "                                  objective= 'binary:logistic', \n",
    "                                  nthread=4, \n",
    "                                  scale_pos_weight=2, \n",
    "                                  seed=27)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = ['mae', 'auc'],\n",
    "                  eval_set = [(train_features, train_labels), (valid_features, valid_labels)],\n",
    "                  #eval_names = ['valid', 'train'],\n",
    "                  early_stopping_rounds = 200, verbose = 200)\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration\n",
    "        \n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        #test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        test_predictions += model.predict_proba(test_features)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.evals_result()['validation_1']['auc'][best_iteration] #model.best_score_['valid']['auc']\n",
    "        train_score = model.evals_result()['validation_1']['auc'][best_iteration] #model.best_score_['train']['auc']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "    return feature_importances, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df):\n",
    "    \"\"\"\n",
    "    Plot importances returned by a model. This can work with any measure of\n",
    "    feature importance provided that higher importance is better. \n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): feature importances. Must have the features in a column\n",
    "        called `features` and the importances in a column called `importance\n",
    "        \n",
    "    Returns:\n",
    "        shows a plot of the 15 most importance features\n",
    "        \n",
    "        df (dataframe): feature importances sorted by importance (highest to lowest) \n",
    "        with a column for normalized importance\n",
    "        \"\"\"\n",
    "    \n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))), \n",
    "            df['importance_normalized'].head(15), \n",
    "            align = 'center', edgecolor = 'k')\n",
    "    \n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "    \n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (215257, 448)\n",
      "Testing Data Shape:  (46127, 448)\n",
      "[0]\tvalidation_0-mae:0.496852\tvalidation_0-auc:0.728675\tvalidation_1-mae:0.496862\tvalidation_1-auc:0.716674\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 200 rounds.\n",
      "[200]\tvalidation_0-mae:0.224135\tvalidation_0-auc:0.782281\tvalidation_1-mae:0.226088\tvalidation_1-auc:0.747693\n",
      "[400]\tvalidation_0-mae:0.183483\tvalidation_0-auc:0.810709\tvalidation_1-mae:0.187295\tvalidation_1-auc:0.754147\n",
      "[600]\tvalidation_0-mae:0.174533\tvalidation_0-auc:0.832586\tvalidation_1-mae:0.180177\tvalidation_1-auc:0.757528\n",
      "[800]\tvalidation_0-mae:0.170849\tvalidation_0-auc:0.848088\tvalidation_1-mae:0.178085\tvalidation_1-auc:0.758713\n",
      "[1000]\tvalidation_0-mae:0.168441\tvalidation_0-auc:0.859704\tvalidation_1-mae:0.176987\tvalidation_1-auc:0.759267\n",
      "[1200]\tvalidation_0-mae:0.166456\tvalidation_0-auc:0.869845\tvalidation_1-mae:0.176184\tvalidation_1-auc:0.759384\n",
      "[1400]\tvalidation_0-mae:0.164384\tvalidation_0-auc:0.880026\tvalidation_1-mae:0.175338\tvalidation_1-auc:0.759488\n",
      "Stopping. Best iteration:\n",
      "[1348]\tvalidation_0-mae:0.164947\tvalidation_0-auc:0.877261\tvalidation_1-mae:0.175564\tvalidation_1-auc:0.759644\n",
      "\n",
      "[0]\tvalidation_0-mae:0.496855\tvalidation_0-auc:0.729919\tvalidation_1-mae:0.496876\tvalidation_1-auc:0.719317\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 200 rounds.\n",
      "[200]\tvalidation_0-mae:0.223519\tvalidation_0-auc:0.783061\tvalidation_1-mae:0.226811\tvalidation_1-auc:0.740706\n",
      "[400]\tvalidation_0-mae:0.18295\tvalidation_0-auc:0.810347\tvalidation_1-mae:0.188051\tvalidation_1-auc:0.747495\n",
      "[600]\tvalidation_0-mae:0.174101\tvalidation_0-auc:0.831567\tvalidation_1-mae:0.180883\tvalidation_1-auc:0.752462\n",
      "[800]\tvalidation_0-mae:0.170609\tvalidation_0-auc:0.846245\tvalidation_1-mae:0.178763\tvalidation_1-auc:0.754221\n",
      "[1000]\tvalidation_0-mae:0.168105\tvalidation_0-auc:0.858669\tvalidation_1-mae:0.17754\tvalidation_1-auc:0.755036\n",
      "[1200]\tvalidation_0-mae:0.166025\tvalidation_0-auc:0.869281\tvalidation_1-mae:0.176673\tvalidation_1-auc:0.755456\n",
      "[1400]\tvalidation_0-mae:0.164028\tvalidation_0-auc:0.879104\tvalidation_1-mae:0.175843\tvalidation_1-auc:0.755813\n",
      "[1600]\tvalidation_0-mae:0.162078\tvalidation_0-auc:0.888144\tvalidation_1-mae:0.175045\tvalidation_1-auc:0.755677\n",
      "Stopping. Best iteration:\n",
      "[1413]\tvalidation_0-mae:0.163907\tvalidation_0-auc:0.879683\tvalidation_1-mae:0.17579\tvalidation_1-auc:0.755854\n",
      "\n",
      "[0]\tvalidation_0-mae:0.496848\tvalidation_0-auc:0.73307\tvalidation_1-mae:0.496859\tvalidation_1-auc:0.702903\n",
      "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-auc hasn't improved in 200 rounds.\n",
      "[200]\tvalidation_0-mae:0.223879\tvalidation_0-auc:0.785345\tvalidation_1-mae:0.226069\tvalidation_1-auc:0.72936\n",
      "[400]\tvalidation_0-mae:0.183156\tvalidation_0-auc:0.813135\tvalidation_1-mae:0.187309\tvalidation_1-auc:0.735901\n",
      "[600]\tvalidation_0-mae:0.174124\tvalidation_0-auc:0.834872\tvalidation_1-mae:0.180206\tvalidation_1-auc:0.740625\n",
      "[800]\tvalidation_0-mae:0.170464\tvalidation_0-auc:0.849091\tvalidation_1-mae:0.177967\tvalidation_1-auc:0.742707\n",
      "[1000]\tvalidation_0-mae:0.167975\tvalidation_0-auc:0.861139\tvalidation_1-mae:0.176809\tvalidation_1-auc:0.74371\n",
      "[1200]\tvalidation_0-mae:0.16604\tvalidation_0-auc:0.87085\tvalidation_1-mae:0.17599\tvalidation_1-auc:0.74432\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"n_estimators\":10000,\n",
    "    \"max_depth\":7,\n",
    "    \"min_child_weight\": 4,\n",
    "    \"subsample\": 0.8, \n",
    "    \"colsample_bytree\": 0.8\n",
    "}\n",
    "\n",
    "fi, metrics = model(app_train, app_test, params=params)\n",
    "print('Baseline metrics')\n",
    "print(metrics)\n",
    "fi_sorted = plot_feature_importances(fi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
