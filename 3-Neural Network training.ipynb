{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#Split-Train/Test/Validation\" data-toc-modified-id=\"Split-Train/Test/Validation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Split Train/Test/Validation</a></span></li><li><span><a href=\"#Baseline-model\" data-toc-modified-id=\"Baseline-model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Baseline model</a></span></li><li><span><a href=\"#Models-Training\" data-toc-modified-id=\"Models-Training-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Models Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Light-Gradient-Boosting-Machine-(LGBM)\" data-toc-modified-id=\"Light-Gradient-Boosting-Machine-(LGBM)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Light Gradient Boosting Machine (LGBM)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T20:12:37.818996Z",
     "start_time": "2018-08-01T20:12:37.445912Z"
    }
   },
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train/Test/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T14:50:42.684435Z",
     "start_time": "2018-08-01T14:49:47.286191Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/eda_dataset_imputed.csv')\n",
    "\n",
    "app_train, test_1 = train_test_split(data, test_size=0.30, random_state=64)\n",
    "app_test, app_validation = train_test_split(test_1, test_size=0.5, random_state=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T14:50:42.715385Z",
     "start_time": "2018-08-01T14:50:42.687230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((215257, 450), (46127, 450), (46127, 450))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_train.shape, app_test.shape, app_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "import gc\n",
    "\n",
    "def model(model_func, features, test_features, params, validation_features=None, n_folds = 5): \n",
    "    \"\"\"Train, test and validation a model using cross validation. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    if validation_features is not None:\n",
    "        validation_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = np.array(features['TARGET'].astype(int))\n",
    "    test_labels = np.array(test_features['TARGET'].astype(int))\n",
    "    if validation_features is not None:\n",
    "        validation_labels = np.array(validation_features['TARGET'].astype(int))\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    features = features.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
    "    test_features = test_features.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
    "    if validation_features is not None:\n",
    "        validation_features = validation_features.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
    "    \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    if validation_features is not None:\n",
    "        print('Validation Data Shape: ', test_features.shape)\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    if validation_features is not None:\n",
    "        validation_features = np.array(validation_features)\n",
    "        \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    if validation_features is not None:\n",
    "        validation_predictions = np.zeros(validation_features.shape[0])\n",
    "        \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores_auc = []\n",
    "    train_scores_auc = []\n",
    "    valid_scores_mae = []\n",
    "    train_scores_mae = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        valid_score_auc, train_score_auc, valid_score_mae, train_score_mae, feature_importance_values_l, test_predictions_l, validation_predictions_l = model_func(train_features, train_labels, valid_features, valid_labels, test_features, validation_features, params)\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importance_values += feature_importance_values_l / k_fold.n_splits\n",
    "\n",
    "        # Make predictions\n",
    "        test_predictions += test_predictions_l / k_fold.n_splits\n",
    "        if validation_features is not None:\n",
    "            validation_predictions += validation_predictions_l / k_fold.n_splits\n",
    "        \n",
    "        valid_scores_auc.append(valid_score_auc)\n",
    "        train_scores_auc.append(train_score_auc)\n",
    "        valid_scores_mae.append(valid_score_mae)\n",
    "        train_scores_mae.append(train_score_mae)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(test_labels, test_predictions)\n",
    "    valid_mae = mean_absolute_error(test_labels, test_predictions)\n",
    "    if validation_features is not None:\n",
    "        validation_auc = roc_auc_score(validation_labels, validation_predictions)\n",
    "        validation_mae = mean_absolute_error(validation_labels, validation_predictions)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores_auc.append(valid_auc)\n",
    "    train_scores_auc.append(np.mean(train_scores_auc))\n",
    "    valid_scores_mae.append(valid_mae)\n",
    "    train_scores_mae.append(np.mean(train_scores_mae))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train auc': train_scores_auc,\n",
    "                            'valid auc': valid_scores_auc,\n",
    "                            'train mae': train_scores_mae,\n",
    "                            'valid mae': valid_scores_mae}) \n",
    "\n",
    "    if validation_features is not None:\n",
    "        validation_metrics = pd.DataFrame({'auc': [validation_auc],\n",
    "                                           'mae': [validation_mae]}) \n",
    "    else:\n",
    "        validation_metrics = None\n",
    "        \n",
    "\n",
    "    return feature_importances, metrics, validation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/datos/usuarios/A153445/environments/py_on_steroids/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn import metrics\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "def train_NNClassifier(train_features, train_labels, valid_features, valid_labels, test_features, validation_features, params):\n",
    "    # Create the model\n",
    "    print( 'Setting up neural network...' )\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(units = params['layer1'] , input_dim = 448))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(Dropout(params['dropout']))\n",
    "    nn.add(Dense(units = params['layer2'] ))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(params['dropout']))\n",
    "    nn.add(Dense(units = params['layer3']))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(params['dropout']))\n",
    "    nn.add(Dense(units = params['layer4']))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(params['dropout']))\n",
    "    nn.add(Dense(units = params['layer5']))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(params['dropout']))\n",
    "    nn.add(Dense(1, activation='sigmoid'))\n",
    "    nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['mae', auc])\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=100)]\n",
    "\n",
    "    # Train the model\n",
    "    nn.fit(train_features, train_labels, \n",
    "           validation_data=(valid_features, valid_labels), \n",
    "           epochs=params['epochs'], \n",
    "           callbacks=callbacks,\n",
    "           batch_size=32,\n",
    "           verbose=2)\n",
    "\n",
    "    # Record the feature importances\n",
    "    feature_importance_values = np.zeros(train_features.shape[1])\n",
    "    \n",
    "    # Make predictions\n",
    "    test_predictions = nn.predict(test_features).flatten().clip(0,1)\n",
    "    if validation_features is not None:\n",
    "        validation_predictions = nn.predict(validation_features).flatten().clip(0,1) \n",
    "    else:\n",
    "        validation_predictions=None\n",
    "        \n",
    "    # Record the best score\n",
    "    t_p = nn.predict(train_features).flatten().clip(0,1)\n",
    "    v_p = nn.predict(valid_features).flatten().clip(0,1)\n",
    "    \n",
    "    # Record the best score\n",
    "    valid_score_auc = roc_auc_score(valid_labels, v_p)\n",
    "    train_score_auc = roc_auc_score(train_labels, t_p)\n",
    "    valid_score_mae = mean_absolute_error(valid_labels, v_p)\n",
    "    train_score_mae = mean_absolute_error(train_labels, t_p)\n",
    "\n",
    "    return valid_score_auc, train_score_auc, valid_score_mae, train_score_mae, feature_importance_values, test_predictions, validation_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (215257, 448)\n",
      "Testing Data Shape:  (46127, 448)\n",
      "Setting up neural network...\n",
      "Train on 172205 samples, validate on 43052 samples\n",
      "Epoch 1/1000\n",
      " - 67s - loss: 0.3128 - mean_absolute_error: 0.1752 - auc: 0.4972 - val_loss: 0.2792 - val_mean_absolute_error: 0.1443 - val_auc: 0.5015\n",
      "Epoch 2/1000\n",
      " - 66s - loss: 0.2843 - mean_absolute_error: 0.1521 - auc: 0.5008 - val_loss: 0.2784 - val_mean_absolute_error: 0.1481 - val_auc: 0.5022\n",
      "Epoch 3/1000\n",
      " - 66s - loss: 0.2818 - mean_absolute_error: 0.1505 - auc: 0.5061 - val_loss: 0.2785 - val_mean_absolute_error: 0.1531 - val_auc: 0.5084\n",
      "Epoch 4/1000\n",
      " - 66s - loss: 0.2813 - mean_absolute_error: 0.1501 - auc: 0.5101 - val_loss: 0.2781 - val_mean_absolute_error: 0.1530 - val_auc: 0.5120\n",
      "Epoch 5/1000\n",
      " - 66s - loss: 0.2807 - mean_absolute_error: 0.1496 - auc: 0.5138 - val_loss: 0.2778 - val_mean_absolute_error: 0.1489 - val_auc: 0.5155\n",
      "Epoch 6/1000\n",
      " - 66s - loss: 0.2804 - mean_absolute_error: 0.1491 - auc: 0.5173 - val_loss: 0.2779 - val_mean_absolute_error: 0.1522 - val_auc: 0.5191\n",
      "Epoch 7/1000\n",
      " - 66s - loss: 0.2803 - mean_absolute_error: 0.1492 - auc: 0.5206 - val_loss: 0.2776 - val_mean_absolute_error: 0.1506 - val_auc: 0.5221\n",
      "Epoch 8/1000\n",
      " - 66s - loss: 0.2803 - mean_absolute_error: 0.1491 - auc: 0.5233 - val_loss: 0.2771 - val_mean_absolute_error: 0.1457 - val_auc: 0.5245\n",
      "Epoch 9/1000\n",
      " - 66s - loss: 0.2801 - mean_absolute_error: 0.1489 - auc: 0.5259 - val_loss: 0.2773 - val_mean_absolute_error: 0.1492 - val_auc: 0.5268\n",
      "Epoch 10/1000\n",
      " - 66s - loss: 0.2800 - mean_absolute_error: 0.1486 - auc: 0.5280 - val_loss: 0.2771 - val_mean_absolute_error: 0.1475 - val_auc: 0.5291\n",
      "Epoch 11/1000\n",
      " - 66s - loss: 0.2798 - mean_absolute_error: 0.1487 - auc: 0.5304 - val_loss: 0.2769 - val_mean_absolute_error: 0.1469 - val_auc: 0.5314\n",
      "Epoch 12/1000\n",
      " - 66s - loss: 0.2797 - mean_absolute_error: 0.1485 - auc: 0.5324 - val_loss: 0.2768 - val_mean_absolute_error: 0.1443 - val_auc: 0.5334\n",
      "Epoch 13/1000\n",
      " - 66s - loss: 0.2795 - mean_absolute_error: 0.1481 - auc: 0.5343 - val_loss: 0.2765 - val_mean_absolute_error: 0.1489 - val_auc: 0.5356\n",
      "Epoch 14/1000\n",
      " - 66s - loss: 0.2793 - mean_absolute_error: 0.1486 - auc: 0.5366 - val_loss: 0.2763 - val_mean_absolute_error: 0.1452 - val_auc: 0.5378\n",
      "Epoch 15/1000\n",
      " - 66s - loss: 0.2792 - mean_absolute_error: 0.1483 - auc: 0.5389 - val_loss: 0.2765 - val_mean_absolute_error: 0.1471 - val_auc: 0.5399\n",
      "Epoch 16/1000\n",
      " - 67s - loss: 0.2789 - mean_absolute_error: 0.1482 - auc: 0.5410 - val_loss: 0.2762 - val_mean_absolute_error: 0.1485 - val_auc: 0.5421\n",
      "Epoch 17/1000\n",
      " - 67s - loss: 0.2789 - mean_absolute_error: 0.1482 - auc: 0.5432 - val_loss: 0.2761 - val_mean_absolute_error: 0.1460 - val_auc: 0.5442\n",
      "Epoch 18/1000\n",
      " - 67s - loss: 0.2786 - mean_absolute_error: 0.1479 - auc: 0.5452 - val_loss: 0.2763 - val_mean_absolute_error: 0.1495 - val_auc: 0.5460\n",
      "Epoch 19/1000\n",
      " - 67s - loss: 0.2786 - mean_absolute_error: 0.1484 - auc: 0.5469 - val_loss: 0.2760 - val_mean_absolute_error: 0.1427 - val_auc: 0.5477\n",
      "Epoch 20/1000\n",
      " - 67s - loss: 0.2784 - mean_absolute_error: 0.1476 - auc: 0.5486 - val_loss: 0.2758 - val_mean_absolute_error: 0.1498 - val_auc: 0.5494\n",
      "Epoch 21/1000\n",
      " - 67s - loss: 0.2784 - mean_absolute_error: 0.1483 - auc: 0.5503 - val_loss: 0.2762 - val_mean_absolute_error: 0.1502 - val_auc: 0.5510\n",
      "Epoch 22/1000\n",
      " - 67s - loss: 0.2784 - mean_absolute_error: 0.1479 - auc: 0.5517 - val_loss: 0.2760 - val_mean_absolute_error: 0.1462 - val_auc: 0.5524\n",
      "Epoch 23/1000\n",
      " - 66s - loss: 0.2784 - mean_absolute_error: 0.1480 - auc: 0.5530 - val_loss: 0.2757 - val_mean_absolute_error: 0.1449 - val_auc: 0.5537\n",
      "Epoch 24/1000\n",
      " - 67s - loss: 0.2783 - mean_absolute_error: 0.1479 - auc: 0.5543 - val_loss: 0.2759 - val_mean_absolute_error: 0.1411 - val_auc: 0.5549\n",
      "Epoch 25/1000\n",
      " - 67s - loss: 0.2782 - mean_absolute_error: 0.1478 - auc: 0.5555 - val_loss: 0.2755 - val_mean_absolute_error: 0.1444 - val_auc: 0.5561\n",
      "Epoch 26/1000\n",
      " - 66s - loss: 0.2781 - mean_absolute_error: 0.1480 - auc: 0.5567 - val_loss: 0.2758 - val_mean_absolute_error: 0.1463 - val_auc: 0.5572\n",
      "Epoch 27/1000\n",
      " - 66s - loss: 0.2780 - mean_absolute_error: 0.1478 - auc: 0.5578 - val_loss: 0.2757 - val_mean_absolute_error: 0.1436 - val_auc: 0.5583\n",
      "Epoch 28/1000\n",
      " - 66s - loss: 0.2780 - mean_absolute_error: 0.1479 - auc: 0.5589 - val_loss: 0.2754 - val_mean_absolute_error: 0.1443 - val_auc: 0.5594\n",
      "Epoch 29/1000\n",
      " - 67s - loss: 0.2779 - mean_absolute_error: 0.1479 - auc: 0.5599 - val_loss: 0.2754 - val_mean_absolute_error: 0.1475 - val_auc: 0.5604\n",
      "Epoch 30/1000\n",
      " - 67s - loss: 0.2783 - mean_absolute_error: 0.1479 - auc: 0.5608 - val_loss: 0.2749 - val_mean_absolute_error: 0.1448 - val_auc: 0.5612\n",
      "Epoch 31/1000\n",
      " - 66s - loss: 0.2780 - mean_absolute_error: 0.1478 - auc: 0.5617 - val_loss: 0.2756 - val_mean_absolute_error: 0.1470 - val_auc: 0.5620\n",
      "Epoch 32/1000\n",
      " - 66s - loss: 0.2779 - mean_absolute_error: 0.1479 - auc: 0.5624 - val_loss: 0.2756 - val_mean_absolute_error: 0.1467 - val_auc: 0.5628\n",
      "Epoch 33/1000\n",
      " - 66s - loss: 0.2778 - mean_absolute_error: 0.1478 - auc: 0.5632 - val_loss: 0.2755 - val_mean_absolute_error: 0.1489 - val_auc: 0.5636\n",
      "Epoch 34/1000\n",
      " - 67s - loss: 0.2778 - mean_absolute_error: 0.1479 - auc: 0.5640 - val_loss: 0.2760 - val_mean_absolute_error: 0.1460 - val_auc: 0.5643\n",
      "Epoch 35/1000\n",
      " - 67s - loss: 0.2780 - mean_absolute_error: 0.1479 - auc: 0.5646 - val_loss: 0.2755 - val_mean_absolute_error: 0.1438 - val_auc: 0.5649\n",
      "Epoch 36/1000\n",
      " - 67s - loss: 0.2780 - mean_absolute_error: 0.1478 - auc: 0.5652 - val_loss: 0.2753 - val_mean_absolute_error: 0.1442 - val_auc: 0.5655\n",
      "Epoch 37/1000\n",
      " - 67s - loss: 0.2779 - mean_absolute_error: 0.1478 - auc: 0.5658 - val_loss: 0.2763 - val_mean_absolute_error: 0.1474 - val_auc: 0.5660\n",
      "Epoch 38/1000\n",
      " - 67s - loss: 0.2779 - mean_absolute_error: 0.1479 - auc: 0.5663 - val_loss: 0.2752 - val_mean_absolute_error: 0.1475 - val_auc: 0.5665\n",
      "Epoch 39/1000\n",
      " - 67s - loss: 0.2777 - mean_absolute_error: 0.1477 - auc: 0.5669 - val_loss: 0.2764 - val_mean_absolute_error: 0.1483 - val_auc: 0.5671\n",
      "Epoch 40/1000\n",
      " - 66s - loss: 0.2776 - mean_absolute_error: 0.1479 - auc: 0.5674 - val_loss: 0.2752 - val_mean_absolute_error: 0.1462 - val_auc: 0.5677\n",
      "Epoch 41/1000\n",
      " - 66s - loss: 0.2778 - mean_absolute_error: 0.1476 - auc: 0.5680 - val_loss: 0.2757 - val_mean_absolute_error: 0.1499 - val_auc: 0.5682\n",
      "Epoch 42/1000\n",
      " - 67s - loss: 0.2778 - mean_absolute_error: 0.1479 - auc: 0.5685 - val_loss: 0.2756 - val_mean_absolute_error: 0.1468 - val_auc: 0.5686\n",
      "Epoch 43/1000\n",
      " - 66s - loss: 0.2777 - mean_absolute_error: 0.1478 - auc: 0.5689 - val_loss: 0.2752 - val_mean_absolute_error: 0.1452 - val_auc: 0.5691\n",
      "Epoch 44/1000\n",
      " - 66s - loss: 0.2777 - mean_absolute_error: 0.1479 - auc: 0.5694 - val_loss: 0.2755 - val_mean_absolute_error: 0.1460 - val_auc: 0.5696\n",
      "Epoch 45/1000\n",
      " - 67s - loss: 0.2776 - mean_absolute_error: 0.1477 - auc: 0.5699 - val_loss: 0.2765 - val_mean_absolute_error: 0.1446 - val_auc: 0.5700\n",
      "Epoch 46/1000\n",
      " - 67s - loss: 0.2776 - mean_absolute_error: 0.1476 - auc: 0.5702 - val_loss: 0.2753 - val_mean_absolute_error: 0.1422 - val_auc: 0.5704\n",
      "Epoch 47/1000\n",
      " - 66s - loss: 0.2775 - mean_absolute_error: 0.1478 - auc: 0.5707 - val_loss: 0.2759 - val_mean_absolute_error: 0.1466 - val_auc: 0.5709\n",
      "Epoch 48/1000\n",
      " - 66s - loss: 0.2775 - mean_absolute_error: 0.1475 - auc: 0.5711 - val_loss: 0.2756 - val_mean_absolute_error: 0.1438 - val_auc: 0.5713\n",
      "Epoch 49/1000\n",
      " - 66s - loss: 0.2775 - mean_absolute_error: 0.1477 - auc: 0.5715 - val_loss: 0.2757 - val_mean_absolute_error: 0.1472 - val_auc: 0.5717\n",
      "Epoch 50/1000\n",
      " - 66s - loss: 0.2776 - mean_absolute_error: 0.1477 - auc: 0.5719 - val_loss: 0.2759 - val_mean_absolute_error: 0.1508 - val_auc: 0.5720\n",
      "Epoch 51/1000\n",
      " - 66s - loss: 0.2773 - mean_absolute_error: 0.1478 - auc: 0.5722 - val_loss: 0.2752 - val_mean_absolute_error: 0.1445 - val_auc: 0.5724\n",
      "Epoch 52/1000\n",
      " - 67s - loss: 0.2774 - mean_absolute_error: 0.1476 - auc: 0.5726 - val_loss: 0.2753 - val_mean_absolute_error: 0.1501 - val_auc: 0.5728\n",
      "Epoch 53/1000\n",
      " - 67s - loss: 0.2773 - mean_absolute_error: 0.1476 - auc: 0.5730 - val_loss: 0.2755 - val_mean_absolute_error: 0.1454 - val_auc: 0.5732\n",
      "Epoch 54/1000\n",
      " - 67s - loss: 0.2774 - mean_absolute_error: 0.1478 - auc: 0.5734 - val_loss: 0.2756 - val_mean_absolute_error: 0.1424 - val_auc: 0.5735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/1000\n",
      " - 67s - loss: 0.2773 - mean_absolute_error: 0.1476 - auc: 0.5737 - val_loss: 0.2755 - val_mean_absolute_error: 0.1413 - val_auc: 0.5739\n",
      "Epoch 56/1000\n",
      " - 66s - loss: 0.2773 - mean_absolute_error: 0.1477 - auc: 0.5740 - val_loss: 0.2751 - val_mean_absolute_error: 0.1461 - val_auc: 0.5742\n",
      "Epoch 57/1000\n",
      " - 66s - loss: 0.2772 - mean_absolute_error: 0.1475 - auc: 0.5744 - val_loss: 0.2753 - val_mean_absolute_error: 0.1451 - val_auc: 0.5745\n",
      "Epoch 58/1000\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"layer1\": 400,\n",
    "    \"layer2\": 160,\n",
    "    \"layer3\": 64,\n",
    "    \"layer4\": 26,\n",
    "    \"layer5\": 12,\n",
    "    \"epochs\": 1000,\n",
    "    \"dropout\": 0.3\n",
    "}\n",
    "\n",
    "fi, metrics, validation_metrics = model(train_NNClassifier, app_train, app_test, params=params)\n",
    "print('Baseline metrics')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"layer1\": 600,\n",
    "    \"layer2\": 300,\n",
    "    \"layer3\": 150,\n",
    "    \"layer4\": 80,\n",
    "    \"layer5\": 20,\n",
    "    \"epochs\": 1000,\n",
    "    \"dropout\": 0.3\n",
    "}\n",
    "\n",
    "fi, metrics, validation_metrics = model(train_NNClassifier, app_train, app_test, params=params)\n",
    "print('Baseline metrics')\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
