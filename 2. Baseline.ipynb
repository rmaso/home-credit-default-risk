{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#Split-Train/Test/Validation\" data-toc-modified-id=\"Split-Train/Test/Validation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Split Train/Test/Validation</a></span></li><li><span><a href=\"#Baseline-model\" data-toc-modified-id=\"Baseline-model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Baseline model</a></span></li><li><span><a href=\"#Models-Training\" data-toc-modified-id=\"Models-Training-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Models Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Light-Gradient-Boosting-Machine-(LGBM)\" data-toc-modified-id=\"Light-Gradient-Boosting-Machine-(LGBM)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Light Gradient Boosting Machine (LGBM)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T20:09:47.751546Z",
     "start_time": "2018-07-31T20:09:46.169243Z"
    }
   },
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train/Test/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T20:10:38.742467Z",
     "start_time": "2018-07-31T20:09:47.754472Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/eda_dataset_imputed.csv')\n",
    "\n",
    "app_train, test_1 = train_test_split(data, test_size=0.30, random_state=64)\n",
    "app_test, app_validation = train_test_split(test_1, test_size=0.5, random_state=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T20:10:38.771698Z",
     "start_time": "2018-07-31T20:10:38.745309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((215257, 451), (46127, 451), (46127, 451))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_train.shape, app_test.shape, app_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T20:10:38.780437Z",
     "start_time": "2018-07-31T20:10:38.774464Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels = app_train['TARGET']\n",
    "test_labels = app_test['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T20:11:30.314442Z",
     "start_time": "2018-07-31T20:10:38.783257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (215257, 450)\n",
      "Testing data shape:  (46127, 450)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "\n",
    "# Drop the target from the training data\n",
    "if 'TARGET' in app_train:\n",
    "    train = app_train.drop(columns = ['TARGET'])\n",
    "else:\n",
    "    train = app_train.copy()\n",
    "\n",
    "    \n",
    "if 'TARGET' in app_test:\n",
    "    test = app_test.drop(columns = ['TARGET'])\n",
    "else:\n",
    "    test = app_test.copy()\n",
    "\n",
    "    \n",
    "# Feature names\n",
    "features = list(train.columns)\n",
    "\n",
    "# Median imputation of missing values\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "# Scale each feature to 0-1\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "# Fit on the training data\n",
    "imputer.fit(train)\n",
    "\n",
    "# Transform both training and testing data\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(test)\n",
    "\n",
    "# Repeat with the scaler\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "print('Training data shape: ', train.shape)\n",
    "print('Testing data shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T20:11:37.630184Z",
     "start_time": "2018-07-31T20:11:30.321827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Make the model with the specified regularization parameter\n",
    "log_reg = LogisticRegression(C = 0.0001)\n",
    "\n",
    "# Train on the training data\n",
    "log_reg.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T20:11:37.715568Z",
     "start_time": "2018-07-31T20:11:37.633614Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "# Make sure to select the second column only\n",
    "log_reg_pred = log_reg.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T20:11:37.767054Z",
     "start_time": "2018-07-31T20:11:37.718495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7109988292401902"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(test_labels, log_reg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T20:11:37.797022Z",
     "start_time": "2018-07-31T20:11:37.779009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14895447690010147"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(test_labels, log_reg_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T20:04:07.636114Z",
     "start_time": "2018-07-31T20:04:07.624940Z"
    }
   },
   "source": [
    "## Light Gradient Boosting Machine (LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T20:55:37.635848Z",
     "start_time": "2018-07-31T20:55:36.299542Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "import gc\n",
    "\n",
    "def model(features, test_features, n_folds = 5): \n",
    "    \"\"\"Train and test a light gradient boosting model using\n",
    "    cross validation. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        features (pd.DataFrame): \n",
    "            dataframe of training features to use \n",
    "            for training a model. Must include the TARGET column.\n",
    "        test_features (pd.DataFrame): \n",
    "            dataframe of testing features to use\n",
    "            for making predictions with the model. \n",
    "        encoding (str, default = 'ohe'): \n",
    "            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n",
    "            n_folds (int, default = 5): number of folds to use for cross validation\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        submission (pd.DataFrame): \n",
    "            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n",
    "            predicted by the model.\n",
    "        feature_importances (pd.DataFrame): \n",
    "            dataframe with the feature importances from the model.\n",
    "        valid_metrics (pd.DataFrame): \n",
    "            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the ids\n",
    "    train_ids = features['SK_ID_CURR']\n",
    "    test_ids = test_features['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = np.array(features['TARGET'].astype(int))\n",
    "    \n",
    "    # Remove the ids and target\n",
    "    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    test_features = test_features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "    \n",
    "    print('Training Data Shape: ', features.shape)\n",
    "    print('Testing Data Shape: ', test_features.shape)\n",
    "    \n",
    "    # Extract feature names\n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    # Convert to np arrays\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    \n",
    "    # Create the kfold object\n",
    "    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n",
    "    \n",
    "    # Empty array for feature importances\n",
    "    feature_importance_values = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Empty array for test predictions\n",
    "    test_predictions = np.zeros(test_features.shape[0])\n",
    "    \n",
    "    # Empty array for out of fold validation predictions\n",
    "    out_of_fold = np.zeros(features.shape[0])\n",
    "    \n",
    "    # Lists for recording validation and training scores\n",
    "    valid_scores = []\n",
    "    train_scores = []\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for train_indices, valid_indices in k_fold.split(features):\n",
    "        \n",
    "        # Training data for the fold\n",
    "        train_features, train_labels = features[train_indices], labels[train_indices]\n",
    "        # Validation data for the fold\n",
    "        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n",
    "        \n",
    "        print(type_of_target(labels[train_indices]))\n",
    "        # Create the model\n",
    "        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n",
    "                                   class_weight = 'balanced', learning_rate = 0.05, \n",
    "                                   reg_alpha = 0.1, reg_lambda = 0.1, \n",
    "                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(train_features, train_labels, eval_metric = 'auc',\n",
    "                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n",
    "                  eval_names = ['valid', 'train'], categorical_feature = [],\n",
    "                  early_stopping_rounds = 100, verbose = 200)\n",
    "        \n",
    "        # Record the best iteration\n",
    "        best_iteration = model.best_iteration_\n",
    "        \n",
    "        # Record the feature importances\n",
    "        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n",
    "        \n",
    "        # Record the out of fold predictions\n",
    "        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n",
    "        \n",
    "        # Record the best score\n",
    "        valid_score = model.best_score_['valid']['auc']\n",
    "        train_score = model.best_score_['train']['auc']\n",
    "        \n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.enable()\n",
    "        del model, train_features, valid_features\n",
    "        gc.collect()\n",
    "        \n",
    "    # Make the feature importance dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n",
    "    \n",
    "    # Overall validation score\n",
    "    valid_auc = roc_auc_score(labels, out_of_fold)\n",
    "    \n",
    "    # Add the overall scores to the metrics\n",
    "    valid_scores.append(valid_auc)\n",
    "    train_scores.append(np.mean(train_scores))\n",
    "    \n",
    "    # Needed for creating dataframe of validation scores\n",
    "    fold_names = list(range(n_folds))\n",
    "    fold_names.append('overall')\n",
    "    \n",
    "    # Dataframe of validation scores\n",
    "    metrics = pd.DataFrame({'fold': fold_names,\n",
    "                            'train': train_scores,\n",
    "                            'valid': valid_scores}) \n",
    "    \n",
    "    return feature_importances, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T20:57:15.535972Z",
     "start_time": "2018-07-31T20:55:37.639112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  (215257, 449)\n",
      "Testing Data Shape:  (46127, 449)\n",
      "binary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ruben/Documents/Desenvolupament/jupyter/venv/lib/python3.6/site-packages/lightgbm/basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is []\n",
      "  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid's auc: 0.758324\ttrain's auc: 0.811642\n",
      "Early stopping, best iteration is:\n",
      "[168]\tvalid's auc: 0.758682\ttrain's auc: 0.803306\n",
      "binary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ruben/Documents/Desenvolupament/jupyter/venv/lib/python3.6/site-packages/lightgbm/basic.py:1042: UserWarning: categorical_feature in Dataset is overridden. New categorical_feature is []\n",
      "  warnings.warn('categorical_feature in Dataset is overridden. New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-91ff2216f5b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapp_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Baseline metrics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-c0ad0d5f5020>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(features, test_features, n_folds)\u001b[0m\n\u001b[1;32m     90\u001b[0m                   \u001b[0meval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                   \u001b[0meval_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                   early_stopping_rounds = 100, verbose = 200)\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Record the best iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Desenvolupament/jupyter/venv/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    693\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Desenvolupament/jupyter/venv/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    472\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Desenvolupament/jupyter/venv/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    202\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Desenvolupament/jupyter/venv/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1526\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1527\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1528\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1529\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fi, metrics = model(app_train, app_test)\n",
    "print('Baseline metrics')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_sorted = plot_feature_importances(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T20:44:35.735252Z",
     "start_time": "2018-07-31T20:44:35.683847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'binary'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "check_classification_targets(app_train['TARGET'])\n",
    "type_of_target(app_train['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T20:52:40.025215Z",
     "start_time": "2018-07-31T20:52:40.014317Z"
    }
   },
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits = 2, shuffle = True, random_state = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-31T20:55:09.913183Z",
     "start_time": "2018-07-31T20:55:09.866679Z"
    }
   },
   "outputs": [],
   "source": [
    "for train_indices, valid_indices in k_fold.split(app_train):\n",
    "    np.array(app_train['TARGET'])[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
